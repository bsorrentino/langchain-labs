{
    "cells": [
        {
            "language": "markdown",
            "source": [
                "# Agent Executor From Scratch\n\nIn this notebook we will go over how to build a basic agent executor from scratch.\n\n# Setup\n\nFirst we need to install the packages required\n\n`yarn add langchain @langchain/openai @langchain/langgraph`\n\nNext, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the search tool we will use)\n\n```\nexport OPENAI_API_KEY=xxxx\nexport TAVILY_API_KEY=xxxx\n```\nOptionally, we can set API key for LangSmith tracing, which will give us best-in-class observability.\n\n```\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=xxxxx\n```"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Create the LangChain agent\n\nFirst, we will create the LangChain agent. For more information on LangChain agents, see this documentation."
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "import dotenv from 'dotenv'\n\nconst result = dotenv.config({ path: '../../.env' })"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "import { pull } from \"langchain/hub\";\nimport { createOpenAIFunctionsAgent } from \"langchain/agents\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst tools = [new TavilySearchResults({ maxResults: 1 })];\n\n// Get the prompt to use - you can modify this!\nconst prompt = await pull<ChatPromptTemplate>(\n  \"hwchase17/openai-functions-agent\"\n);\n\nconsole.log( prompt )\n\n// Choose the LLM that will drive the agent\nconst llm = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo-1106\",\n  temperature: 0,\n  verbose: true\n});\n\n// Construct the OpenAI Functions agent\nconst agentRunnable = await createOpenAIFunctionsAgent({\n  llm,\n  tools,\n  prompt\n});"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Define the graph schema\n\nWe now define the graph state. The state for the traditional LangChain agent has a few attributes:\n\n1. **input**: \n   > _This is the input string representing the main ask from the user, passed in as input._\n1. **chatHistory**:\n   > _This is any previous conversation messages, also passed in as input._\n1. **steps**: \n   >  _This is list of actions and corresponding observations that the agent takes over time. This is updated each iteration of the agent._\n1. **agentOutcome**: \n   > _This is the response from the agent, either an AgentAction or AgentFinish. The AgentExecutor should finish when this is an AgentFinish, otherwise it should call the requested tools._"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "const agentState = {\n    input: {\n      value: null\n    },\n    chatHistory: {\n      value: null,\n      default: () => []\n    },\n    steps: {\n      value: (x, y) => x.concat(y),\n      default: () => []\n    },\n    agentOutcome: {\n      value: null\n    }\n  };"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Define the nodes\n\nWe now need to define a few different nodes in our graph. In langgraph, a node can be either a function or a runnable. There are two main nodes we need for this:\n\n1. **The agent**: \n   > _responsible for deciding what (if any) actions to take._\n1. **A function to invoke tools**: \n   > _if the agent decides to take an action, this node will then execute that action._\n\nWe will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides).\n\n1. **Conditional Edge**\n   > _after the agent is called, we should either: a. If the agent said to take an action, then the function to invoke tools should be called b. If the agent said that it was finished, then it should finish_\n1. **Normal Edge**: \n   > _after the tools are invoked, it should always go back to the agent to decide what to do next_\n\n\nLet's define the nodes, as well as a function to decide how what conditional edge to take."
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { BaseMessage } from \"@langchain/core/messages\";\nimport { AgentAction, AgentFinish, AgentStep } from \"@langchain/core/agents\";\nimport { ToolExecutor } from \"@langchain/langgraph/prebuilt\";\nimport type { RunnableConfig } from \"@langchain/core/runnables\";\n\ninterface AgentStateBase {\n  agentOutcome?: AgentAction | AgentFinish;\n  steps: Array<AgentStep>;\n}\n\ninterface AgentState extends AgentStateBase {\n  input: string;\n  chatHistory?: BaseMessage[];\n}\n\nconst toolExecutor = new ToolExecutor({\n  tools,\n});\n\n// Define logic that will be used to determine which conditional edge to go down\nconst shouldContinue = (data: AgentState) => {\n  if (data.agentOutcome && \"returnValues\" in data.agentOutcome) {\n    return \"end\";\n  }\n  return \"continue\";\n};\n\nconst runAgent = async (data: AgentState, config?: RunnableConfig) => {\n  const agentOutcome = await agentRunnable.invoke(data, config);\n  return {\n    agentOutcome,\n  };\n};\n\nconst executeTools = async (data: AgentState, config?: RunnableConfig) => {\n  const agentAction = data.agentOutcome;\n  if (!agentAction || \"returnValues\" in agentAction) {\n    throw new Error(\"Agent has not been run yet\");\n  }\n  const output = await toolExecutor.invoke(agentAction, config);\n  return {\n    steps: [{ action: agentAction, observation: JSON.stringify(output) }]\n  };\n};"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Define the graph\n\nWe can now put it all together and define the graph!"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "import { RunnableLambda } from \"@langchain/core/runnables\";\nimport { END, StateGraph } from \"@langchain/langgraph\";\n\n// Define a new graph\nconst workflow = new StateGraph({\n    channels: agentState\n});\n\n// Define the two nodes we will cycle between\nworkflow.addNode(\"agent\", new RunnableLambda({ func: runAgent }));\nworkflow.addNode(\"action\", new RunnableLambda({ func: executeTools }));\n\n// Set the entrypoint as `agent`\n// This means that this node is the first one called\nworkflow.setEntryPoint(\"agent\");\n\n// We now add a conditional edge\nworkflow.addConditionalEdges(\n  // First, we define the start node. We use `agent`.\n  // This means these are the edges taken after the `agent` node is called.\n  \"agent\",\n  // Next, we pass in the function that will determine which node is called next.\n  shouldContinue,\n  // Finally we pass in a mapping.\n  // The keys are strings, and the values are other nodes.\n  // END is a special node marking that the graph should finish.\n  // What will happen is we will call `should_continue`, and then the output of that\n  // will be matched against the keys in this mapping.\n  // Based on which one it matches, that node will then be called.\n  {\n    // If `tools`, then we call the tool node.\n    continue: \"action\",\n    // Otherwise we finish.\n    end: END\n  }\n);\n\n// We now add a normal edge from `tools` to `agent`.\n// This means that after `tools` is called, `agent` node is called next.\nworkflow.addEdge(\"action\", \"agent\");\n\nconst app = workflow.compile();\n"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "const inputs = { input: \"what is the weather in sf\" }\nfor await (const s of await app.stream(inputs)) {\n  console.log(s)\n  console.log(\"----\\n\")\n}"
            ],
            "outputs": []
        }
    ]
}