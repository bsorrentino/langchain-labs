{
    "cells": [
        {
            "language": "markdown",
            "source": [
                "# Agent Executor From Scratch\n\nIn this notebook we will go over how to build a basic agent executor from scratch.\n\n# Setup\n\nFirst we need to install the packages required\n\n`yarn add langchain @langchain/openai @langchain/langgraph`\n\nNext, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the search tool we will use)\n\n```\nexport OPENAI_API_KEY=xxxx\n```\nOptionally, we can set API key for LangSmith tracing, which will give us best-in-class observability.\n\n```\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=xxxxx\n```"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Create the LangChain agent\n\nFirst, we will create the LangChain agent. For more information on LangChain agents, see this documentation."
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "import dotenv from 'dotenv'\n\nconst result = dotenv.config({ path: '../.env' })"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "Create Command Executor tool"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { Tool, ToolParams } from \"langchain/tools\";\nimport { CallbackManagerForToolRun } from \"@langchain/core/callbacks/manager\";\n\n/**\n * SystemCommandTool is a Tool subclass that allows executing arbitrary \n * system commands. It takes a command string as input and executes it using \n * Node's child_process.spawn functionality.\n*/\nclass SystemCommandTool extends Tool {\n    name =\"system_cmd\"\n    description = \"all system commands\"\n    \n    constructor(fields?: ToolParams) {\n        super( fields )\n        this.returnDirect = true\n    }\n    protected async _call(args: any, runManager?: CallbackManagerForToolRun | undefined): Promise<string> {\n      \n        console.log(\"running system command tool\", args)\n        return `command executed: ${args}`\n\n    }\n  }"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "import { pull } from \"langchain/hub\";\nimport { createOpenAIFunctionsAgent } from \"langchain/agents\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\nimport os from 'node:os' \n\nconst tools = [new SystemCommandTool()];\n\n// Get the prompt to use - you can modify this!\nlet prompt = await pull<ChatPromptTemplate>(\n  \"bsorrentino/copilot-cli-agent\"\n)\n\nprompt = await prompt.partial( { platform: os.platform() } );\n\n// Choose the LLM that will drive the agent\nconst llm = new ChatOpenAI({\n  modelName: \"gpt-4-1106-preview\",\n  temperature: 0\n});\n\n// Construct the OpenAI Functions agent\nconst agentRunnable = await createOpenAIFunctionsAgent({\n  llm,\n  tools,\n  prompt\n});"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Define the graph schema\n\nWe now define the graph state. The state for the traditional LangChain agent has a few attributes:\n\n1. **input**: \n   > _This is the input string representing the main ask from the user, passed in as input._\n1. **chatHistory**:\n   > _This is any previous conversation messages, also passed in as input._\n1. **steps**: \n   >  _This is list of actions and corresponding observations that the agent takes over time. This is updated each iteration of the agent._\n1. **agentOutcome**: \n   > _This is the response from the agent, either an AgentAction or AgentFinish. The AgentExecutor should finish when this is an AgentFinish, otherwise it should call the requested tools._"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "\nconst agentState = {\n    input: {\n      value: null\n    },\n    chatHistory: {\n      value: null,\n      default: () => []\n    },\n    steps: {\n      value: (x, y) => x.concat(y),\n      default: () => []\n    },\n    agentOutcome: {\n      value: null\n    }\n  };"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Define the nodes\n\nWe now need to define a few different nodes in our graph. In langgraph, a node can be either a function or a runnable. There are two main nodes we need for this:\n\n1. **The agent**: \n   > _responsible for deciding what (if any) actions to take._\n1. **A function to invoke tools**: \n   > _if the agent decides to take an action, this node will then execute that action._\n\nWe will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides).\n\n1. **Conditional Edge**\n   > _after the agent is called, we should either: a. If the agent said to take an action, then the function to invoke tools should be called b. If the agent said that it was finished, then it should finish_\n1. **Normal Edge**: \n   > _after the tools are invoked, it should always go back to the agent to decide what to do next_\n\n\nLet's define the nodes, as well as a function to decide how what conditional edge to take."
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { BaseMessage } from \"@langchain/core/messages\";\nimport { AgentAction, AgentFinish, AgentStep } from \"@langchain/core/agents\";\nimport { ToolExecutor } from \"@langchain/langgraph/prebuilt\";\nimport type { RunnableConfig } from \"@langchain/core/runnables\";\nimport { END } from \"@langchain/langgraph\";\n\ninterface AgentStateBase {\n  agentOutcome?: AgentAction | AgentFinish;\n  steps: Array<AgentStep>;\n}\n\ninterface AgentState extends AgentStateBase {\n  input: string;\n  platform: string,\n  chatHistory?: BaseMessage[];\n}\n\nconst toolExecutor = new ToolExecutor({tools});\n\n// Define logic that will be used to determine which conditional edge to go down\nconst shouldContinue = (data: AgentState) => {\n  if (data.agentOutcome && \"returnValues\" in data.agentOutcome) {\n    return \"end\";\n  }\n  return \"continue\";\n};\n\nconst runAgent = async (data: AgentState, config?: RunnableConfig) => {\n  const agentOutcome = await agentRunnable.invoke(data, config);\n\n  return {\n    agentOutcome,\n  };\n};\n\n\n\nconst executeTools = async (data: AgentState, config?: RunnableConfig) => {\n  const agentAction = data.agentOutcome;\n\n  if (!agentAction || \"returnValues\" in agentAction) {\n    throw new Error(\"Agent has not been run yet\");\n  }\n\n  const output = await toolExecutor.invoke(agentAction, config);\n\n  return {\n    steps: [{ action: agentAction, observation: JSON.stringify(output) }]\n  };\n};"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "## Define the graph\n\nWe can now put it all together and define the graph!"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "import { RunnableLambda } from \"@langchain/core/runnables\";\nimport { END, StateGraph } from \"@langchain/langgraph\";\n\n// Define a new graph\nconst workflow = new StateGraph({\n    channels: agentState\n});\n\n// Define the two nodes we will cycle between\nworkflow.addNode(\"agent\", new RunnableLambda({ func: runAgent }));\nworkflow.addNode(\"action\", new RunnableLambda({ func: executeTools }));\n\n// Set the entrypoint as `agent`\n// This means that this node is the first one called\nworkflow.setEntryPoint(\"agent\");\n\n// We now add a conditional edge\nworkflow.addConditionalEdges(\n  // First, we define the start node. We use `agent`.\n  // This means these are the edges taken after the `agent` node is called.\n  \"agent\",\n  // Next, we pass in the function that will determine which node is called next.\n  shouldContinue,\n  // Finally we pass in a mapping.\n  // The keys are strings, and the values are other nodes.\n  // END is a special node marking that the graph should finish.\n  // What will happen is we will call `should_continue`, and then the output of that\n  // will be matched against the keys in this mapping.\n  // Based on which one it matches, that node will then be called.\n  {\n    // If `tools`, then we call the tool node.\n    continue: \"action\",\n    // Otherwise we finish.\n    end: END\n  }\n);\n\n// workflow.addConditionalEdges(\"action\", ( data ) =>  \"end\",\n//  {\n//   continue: \"agent\",\n//   end: END\n// });\n\n// We now add a normal edge from `tools` to `agent`.\n// This means that after `tools` is called, `agent` node is called next.\nworkflow.addEdge(\"action\", \"agent\");\n\nconst app = workflow.compile();\n"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "\nconst inputs = { input: \"list files in 'downloads' folder and for each create a symbolic link in the '/tmp' directory \" }\nfor await (const s of await app.stream(inputs)) {\n  console.log(s)\n  console.log(\"----\\n\")\n}"
            ],
            "outputs": [
                {
                    "items": [
                        {
                            "mime": "application/vnd.code.notebook.stdout",
                            "value": [
                                "{",
                                "  agent: {",
                                "    agentOutcome: {",
                                "      tool: 'system_cmd',",
                                "      toolInput: [Object],",
                                "      log: 'Invoking \"system_cmd\" with {\"input\":\"ls ~/downloads\"}\\n',",
                                "      messageLog: [Array]",
                                "    }",
                                "  }",
                                "}",
                                "----",
                                "",
                                "running system command tool ls ~/downloads",
                                "{ action: { steps: [ [Object] ] } }",
                                "----",
                                "",
                                "{",
                                "  agent: {",
                                "    agentOutcome: {",
                                "      tool: 'system_cmd',",
                                "      toolInput: [Object],",
                                "      log: 'Invoking \"system_cmd\" with {\"input\":\"ln -s ~/downloads/* /tmp/\"}\\n',",
                                "      messageLog: [Array]",
                                "    }",
                                "  }",
                                "}",
                                "----",
                                "",
                                "running system command tool ln -s ~/downloads/* /tmp/",
                                "{ action: { steps: [ [Object] ] } }",
                                "----",
                                "",
                                "{",
                                "  agent: { agentOutcome: { returnValues: [Object], log: 'Completed' } }",
                                "}",
                                "----",
                                "",
                                "{",
                                "  __end__: {",
                                "    input: \"list files in 'downloads' folder and for each create a symbolic link in the '/tmp' directory \",",
                                "    chatHistory: null,",
                                "    steps: [ [Object], [Object] ],",
                                "    agentOutcome: { returnValues: [Object], log: 'Completed' }",
                                "  }",
                                "}",
                                "----",
                                "",
                                ""
                            ]
                        }
                    ]
                }
            ]
        }
    ]
}