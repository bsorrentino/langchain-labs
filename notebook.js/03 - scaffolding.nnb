{
    "cells": [
        {
            "language": "typescript",
            "source": [
                "import dotenv from 'dotenv'\n\nconst result = dotenv.config({ path: '../.env' })"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "define prompts"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "const promptGenerateToolTemplate =`\nAs my typescript assistant \n\nI need that you create a langchain command tool as a plugin for the copilot-cli-agent application.\nTo do that you must fill the typescript template below with the variables:\nNAME = {name}\nDESC = {desc}\nSCHEMA = {schema}\nand then copy the code below into a file named <NAME>.ts.\n\n\n// beging template\nimport {{ z }} from \"zod\";\nimport {{ CommandTool }} from \"copilot-cli-core\";\n\n<SCHEMA>;\n\nclass \"Camel case of <NAME>\"Tool extends CommandTool<typeof schema> {{\n    name = \"snake case of <NAME>\";\n    description = \"<DESC>\";\n    schema = schema;\n    \n    async _call(arg, runManager) {{\n        console.debug(\"executing <NAME> with arg:\", arg);\n        return \"<NAME> executed!\";\n    }}\n}}\nexport default new \"Camel case of <NAME>\"Tool();\n// end template\n\n`\n\nconst promptGenerateToolTemplateWithCommand =`\n  As my typescript assistant \n\n  I need that you create a langchain command tool as a plugin for the copilot-cli-agent application.\n  To do that you must fill the typescript template below with the variables:\n  NAME = {name}\n  DESC = {desc}\n  SCHEMA = {schema}\n  COMMAND = {command}\n\n  and then copy the code below into a file named <NAME>.mts at {path}.\n\n  before filling the template consider to transform <COMMAND> in a string matching the schema attribute with command parameters.\n\n  as example :\n\n  ls -la <path> must be translated into \"ls -la {{arg.path}}\" \n\n  // beging template\n  import {{ z }} from \"zod\";\n  import {{ CommandTool, expandTilde, runCommand }} from \"copilot-cli-core\";\n\n  <SCHEMA>;\n\n  class \"Camel Case of <NAME>\"Tool extends CommandTool<typeof schema> {{\n      name = \"Snake case of <NAME>\";\n      description = \"<DESC>\";\n      schema = schema;\n      \n      async _call(arg: z.output<typeof schema>) {{\n          console.debug(\"executing '<NAME>' with arg:\", arg);\n\n          await runCommand( <command>, this.execContext )\n          return \"<NAME> executed!\";\n      }}\n  }}\n  export default new \"Camel case of <NAME>\"Tool();\n  // end template\n  `\nconst promptZodSchemaTemplate = `\nAs my typescript ASSISTANT expert in Zod notation.\nyou MUST create the typescript code for creating an object schema following the template below:\n\n// beging template\nconst schema = z.object(\n    // here the properties of the schema\n);\n// end template\n\nTo do this you MUST start to ask interactively to the USER the properties attribute of the object schema following the process below\n\n1. ask for property name\n2. ask for property type\n3. ask if it is required or not\n\ncontinue until USER write \"END\" as property name.\n`\n\nconst promptZodSchemaOneShotTemplate =`\nAs my typescript ASSISTANT expert in Zod usage.\nyou MUST create the typescript code for creating an object schema following the template below:\n\n// beging template\nconst schema = z.object(\n    // here the properties of the schema\n);\n// end template\n\nTo do this you MUST start to interact to the USER following the process below\n\n1. ask for properties information\n2. generate the typescript code for the schema\n3. ask to the USER to confirm the generated code\n4. if USER doesn't confirm the generated code, ask for the properties information again otherwise return typescript code \n`"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "Initialize prompt Zod Schema One Shot Template"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { LLMChain } from \"langchain/chains\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\n\nconst chat = new ChatOpenAI({ modelName: \"gpt-4\", temperature: 0});\nconst memory = new BufferMemory({ returnMessages: true, memoryKey: \"history\" })\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [ \"system\", promptZodSchemaOneShotTemplate], \n  new MessagesPlaceholder(\"history\"),\n  [\"human\", \"{input}\"]] )\n\nconst chain = new LLMChain({\n    llm: chat,\n    prompt: prompt,\n    memory: memory,\n  });\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "Initialize Prompt Zod Schema interactively Template"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { LLMChain } from \"langchain/chains\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\n\nconst chat = new ChatOpenAI({ modelName: \"gpt-4\", temperature: 0, stop: [\"END\"] });\nconst memory = new BufferMemory({ returnMessages: true, memoryKey: \"history\" })\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [ \"system\", promptZodSchemaTemplate], \n  new MessagesPlaceholder(\"history\"),\n  [\"human\", \"{input}\"]] )\n\nconsole.log( await prompt.format( { input: \"let's start\", history: [] } ) )\n\nconst chain = new LLMChain({\n    llm: chat,\n    prompt: prompt,\n    memory: memory,\n  });\n\nlet res = await chain.call({  input: `let's start` });\n\nconsole.log( res )\n\nres = await chain.call({  input: `property1` });\n\nconsole.log( res )\n\nres = await chain.call({  input: `string` });\n\nconsole.log( res )\n\nres = await chain.call({  input: `no` });\n\nconsole.log( res )\n\nres = await chain.call({  input: `END` });\n\nconsole.log( res )"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "let's start "
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { LLMChain } from \"langchain/chains\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\ndeclare const chain: LLMChain;\n\nconst res = await chain.call({  \n    input: `properties: imagePath required, outputPath optional`\n});\n\nconst parser = new StringOutputParser();\n\nconst text = await parser.parse(res.text);\nconsole.log(text);\n\nconst pres = /(const schema = z.object\\({.+}\\);)/gms.exec(text)\n\nconst schema = pres ? pres[1] : null;\n\nconsole.log(schema ?? \"not found\");\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "Update schema"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { LLMChain } from \"langchain/chains\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\ndeclare const chain: LLMChain;\n\n// const res = await chain.call({  input: \"yes, it is correct\"});\nconst res = await chain.call({  \n    input: \"No, add new property grayLevel optional as enum with values  4, 8 or 16 with default value 16\" \n});\n\nconst parser = new StringOutputParser();\n\nconst text = await parser.parse(res.text);\nconsole.log(text);\n\nconst pres = /(const schema = z.object\\({.+}\\);)/gms.exec(text)\n\nconst schema = pres ? pres[1] : null;\n\nconsole.log(schema ?? \"not found\");\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "### Tool defining a function that save a file"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { StructuredTool } from \"langchain/tools\";\nimport { z } from \"zod\";\n\nconst SaveFileSchema = z.object({\n  content: z.string().describe(\" text file content\"),\n});\nclass SaveFileTool extends StructuredTool<typeof SaveFileSchema>  {\n  name =\"save_file\"\n  description = \"save source file on local file system\"\n  schema = SaveFileSchema;\n\n  protected async _call(arg: z.output<typeof SaveFileSchema>): Promise<string> {\n    \n    console.log(\"save file\", arg )\n    return \"file saved\"\n\n      // const result = await runCommand( arg, this.execContext )\n      // return `command executed: ${result}`\n\n  }\n}\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "### scaffold the Tool using function calling\n> call a function to save generated code"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {  initializeAgentExecutorWithOptions } from \"langchain/agents\";\n\ndeclare const SaveFileTool:any\n// declare const schema: string\n\nconst schema = `\nconst GrayLevel = z.enum([\"4\", \"8\", \"16\"]); \n\nconst schema = z.object({\n    imagePath: z.string(),\n    outputPath: z.string().optional(),\n    grayLevel: GrayLevel.default(\"16\"),\n});`;\n\n\nconst generateToolClass = async( args:{\n        name: string, \n        desc:string, \n        schema: string, \n        command: string, \n        path:string} ) => {\n  \n  const model = new ChatOpenAI({\n    // modelName: \"gpt-4\",\n    modelName: \"gpt-3.5-turbo-0613\",\n    // stop: [\"end\", \"stop\", \"quit\"],\n    maxConcurrency: 1,\n    maxRetries: 3,\n    maxTokens: 600,\n    temperature: 0\n  });\n  \n  const tools = [ new SaveFileTool() ] ;\n  \n  const agent = await initializeAgentExecutorWithOptions( tools, model, {\n    agentType: \"openai-functions\",\n    verbose: false,\n    handleParsingErrors: (e:any) => \"there is an error!\"\n  });\n  \n  // We can construct an LLMChain from a PromptTemplate and an LLM.\n  \n  const template = PromptTemplate.fromTemplate(\n    promptGenerateToolTemplateWithCommand\n  );\n  \n  const prompt = await template.format(args)\n  \n  return await agent.run(prompt);\n\n}\n\nconst res = await generateToolClass({\n  name: \"test_run\",\n  desc: \"test description\",\n  schema: schema,\n  command: \"plantuml -encodesprite <grayLevel> <imagePath>\",\n  path: \"./plugins/save_file.mts\"\n})\nconsole.log(res);\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "Confirm Schema \n> not really useful"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "\nimport { LLMChain } from \"langchain/chains\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\ndeclare const chain: LLMChain;\n\nconst res = await chain.call({  input: \"yes, it is correct\"});\n\nconst parser = new StringOutputParser();\n\nconst text = await parser.parse(res.text);\nconsole.log(text);\n\nconst pres = /(const schema = z.object\\({.+}\\);)/gms.exec(text)\n\nconst schema = pres ? pres[1] : null;\n\nconsole.log(schema ?? \"not found\");\n"
            ],
            "outputs": []
        },
        {
            "language": "markdown",
            "source": [
                "Scaffold tool not using function calling.\n> This implies that you save file by code"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\n\ndeclare const schema: string;\n\n// We can construct an LLMChain from a PromptTemplate and an LLM.\nconst model = new OpenAI({ temperature: 0 });\nconst prompt = PromptTemplate.fromTemplate(\n  promptGenerateToolTemplate\n);\nconst chainA = new LLMChain({ llm: model, prompt });\n\n// The result is an object with a `text` property.\nconst resA = await chainA.call({ \n  name: \"PlantUML Sprite Maker\",\n  desc: \"Generate a PlantUML sprite from a given image.\",\n  schema: schema,\n});\n\nconsole.log(resA.text);\n\n"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { RegexParser } from \"langchain/output_parsers\";\n\nconst regexp = new RegExp(/```typescript\\s*(?:import { z } from 'zod';)?\\s*(.+)```/, \"s\" );\n\ntry {\n    const parser = new RegexParser( regexp, ['code'], 'noContent' );\n\n    const text = `\n    \\`\\`\\`typescript\n    import { z } from 'zod';\n    \n    const GrayLevelEnum = z.enum(['4', '8', '16']);\n    \n    const schema = z.object({\n        imagePath: z.string(),\n        outputPath: z.string().optional(),\n        grayLevel: GrayLevelEnum.default('16'),\n    });\n    \\`\\`\\`\n    `\n    // console.log(text);\n    \n    const res = await parser.parse( text );\n    \n    console.log( res );\n    \n}\ncatch( e ) {\n    console.error( e );\n}\n    \n"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { z } from 'zod';\n\nconst schema = z.object({\n  primary: z.string(),\n  secondary: z.string().default('value'),\n});\n\nconst data = { primary: 'value' };\nconst result = schema.parse(data);\nconsole.log(result.secondary); // Output: 'value'\n"
            ],
            "outputs": []
        },
        {
            "language": "typescript",
            "source": [
                "import { spawn } from 'node:child_process';\nimport * as fs from 'node:fs'\n\ntype RunCommandArg = { cmd: string, out?: string, err?:string }\n/**\n * Runs a shell command and returns the output.\n * \n * @param cmd - The command to run.\n * @param ctx - Optional execution context for logging output. \n * @returns Promise resolving to the command output string.\n */\nconst runCommand = async ( \n      arg: string | RunCommandArg, ctx?: any) => {\n  \n      \n  let options: RunCommandArg\n\n  if(typeof arg === 'string') {\n    options = { cmd:arg };\n  } else {\n    options = arg\n  }\n\n  return new Promise<string>( (resolve, reject) => {\n    \n    ctx?.setProgress( `Running command: ${options.cmd}` )\n\n    const child =  spawn(options.cmd, { stdio: [ 'inherit', 'pipe', 'pipe'], shell:true } ) \n\n    let result = \"\"\n\n    if( options.out ) {\n        const output = fs.createWriteStream(options.out);\n        child.stdout.pipe(output);\n    }\n//    else {\n        // Read stdout\n        child.stdout.setEncoding('utf8')\n        child.stdout.on('data', data => {\n        result = data.toString()\n        ctx?.log( `^!${options.cmd}`)\n        ctx?.log( result ) \n        });\n    //}\n\n    if( options.err ) {\n        const output = fs.createWriteStream(options.err);\n        child.stderr.pipe(output);\n    }\n    else {\n        // Read stderr\n        child.stderr.setEncoding('utf8')\n        child.stderr.on('data', data => {\n            result = data.toString()\n            ctx?.log( `^R${result}`) ;\n        })\n    }\n\n    // Handle errors\n    child.on('error', error => {\n      ctx?.log( `^!${options.cmd}`)\n      reject(error.message) \n      \n    })\n\n    // Handle process exit\n    child.on('close', code => { \n      resolve(result) \n    });\n\n  })\n}\n\nrunCommand({ cmd:'ls -l' , out: './out.txt' }, \n{\n    setProgress: ( msg:string) => console.log( 'progress', msg ),\n    log: (msg:string) => console.log( msg) \n})"
            ],
            "outputs": []
        },
        {
            "language": "javascript",
            "source": [
                "import path from 'node:path'\n\n// Function to replace the file extension\nfunction replaceFileExtension(filePath, newExtension) {\n    const parsedPath = path.parse(filePath);\n\n    parsedPath.base = `${parsedPath.name}${newExtension}`;\n\n    // Return the new path\n    return path.format( parsedPath );\n}\n\n// Example usage\nconst originalFilePath = 'example/test.txt';\nconst newFilePath = replaceFileExtension(originalFilePath, '.md');\n\nconsole.log(newFilePath); // Outputs: example/test.md\n"
            ],
            "outputs": []
        }
    ]
}